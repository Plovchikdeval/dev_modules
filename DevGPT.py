__version__ = (1, 0, 17)

"""
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•
â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆ   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•  â•šâ•â•â•â•   â•šâ•â•â•â•â• â•šâ•â•     â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•
                                                                     
(C) 2024 t.me/devjmodules
Licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International
"""
# scope: hikka_only
# scope: hikka_min 1.3.3
# meta developer: @devjmodules
# meta banner: https://kappa.lol/nfF_A
# requires: requests

import logging
import io
import os
import inspect
import aiohttp
import json
import requests

from telethon.tl.types import Message

from .. import loader, utils

logger = logging.getLogger(__name__)

@loader.tds
class DevGPT(loader.Module):
	"""DevGPT - Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ°Ğ¼ Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ñ chatgpt Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¾Ñ‚Ğ¾."""

	strings = {
		"name": "DevGPT",
		"wait": "<blockquote><emoji document_id=5994544674604322765>ğŸ˜€</emoji> <b>The server is processing the request, please wait...</b></blockquote>",
		"quest": "\n\n\n<blockquote><emoji document_id=5465143921912846619>ğŸ’­</emoji> <b>Your request:</b> {args}</blockquote>",
		"quest_img": "<blockquote><b><emoji document_id=5877465816030515018>ğŸ˜€</emoji> Link: <a href='{img_url}'>image</a></b></blockquote>\n\n<blockquote><emoji document_id=5465143921912846619>ğŸ’­</emoji> <b>Prompt:</b> <code>{prmpt}</code></blockquote>\n\n<blockquote><emoji document_id=5994544674604322765>ğŸ˜€</emoji> <b>Model:</b> <code>{mdl}</code></blockquote>",
		"args_err": "<blockquote><emoji document_id=5897846616966041652>â“</emoji> <b>Usage: {prefix}dgpt/dimg <model> <request></b></blockquote>",
		"query_err": "<blockquote><emoji document_id=5208434048753484584>â›”</emoji> <b>The request cannot be empty!</b></blockquote>",
		"server_err": "<blockquote><emoji document_id=5881702736843511327>âš ï¸</emoji> <b>Server error: {error}</b></blockquote>",
		"image_err": "<emoji document_id=5881702736843511327>âš ï¸</emoji> <b>Error generating image: {error}</b>",
		"models_list": "<blockquote><emoji document_id=5879841310902324730>ğŸ˜€</emoji><b>Text</b></blockquote>\n\n<blockquote>{txt_models}</blockquote>\n\n<blockquote><emoji document_id=5775949822993371030>ğŸ–¼</emoji> <b>Images</b></blockquote>\n\n<blockquote>{img_models}</blockquote>",
		"model_not_found": "<blockquote><emoji document_id=5208434048753484584>â›”</emoji> <b>Model not found! List of available models: {prefix}dgmodels</b></blockquote>",
		"no_url": "No image URL received",
		"no_server_respond": "No response from the server",
		"fetch_failed": "<blockquote><emoji document_id=5208663713539704322>ğŸ‘</emoji> <b>Fetching data failed</b></blockquote>",
		"actual_version": "<blockquote> <emoji document_id=5208763618773978162>âœ…</emoji>You have actual DevGPT ({ver})</b></blockquote>",
		"old_version": "<blockquote><emoji document_id=5875291072225087249>ğŸ“Š</emoji> You have old DevGPT ({ver}) </b></blockquote>",
		"update_command": "<blockquote><emoji document_id=5877410604225924969>ğŸ”„</emoji> To update type:</b> <code> {prefix}dlm {upd_file}</code>\n\n<emoji document_id=5879883461711367869>â¬‡ï¸</emoji> <b>New version: {new_ver} <b></blockquote>",
		"ban": "<blockquote><emoji document_id=5208663713539704322>ğŸ‘</emoji> You are banned! Reason: {reason}</blockquote>",
	}

	strings_ua = {
		"wait": "<blockquote><emoji document_id=5994544674604322765>ğŸ˜€</emoji> <b>Ğ¡ĞµÑ€Ğ²ĞµÑ€ Ğ¾Ğ±Ñ€Ğ¾Ğ±Ğ»ÑÑ” Ğ·Ğ°Ğ¿Ğ¸Ñ‚, Ğ±ÑƒĞ´ÑŒ Ğ»Ğ°ÑĞºĞ°, Ğ·Ğ°Ñ‡ĞµĞºĞ°Ğ¹Ñ‚Ğµ...</b></blockquote>",
		"quest": "\n\n\n<blockquote><emoji document_id=5465143921912846619>ğŸ’­</emoji> <b>Ğ’Ğ°Ñˆ Ğ·Ğ°Ğ¿Ğ¸Ñ‚:</b> {args}</blockquote>",
		"quest_img": "<blockquote><b><emoji document_id=5877465816030515018>ğŸ˜€</emoji> ĞŸĞ¾ÑĞ¸Ğ»Ğ°Ğ½Ğ½Ñ: <a href='{img_url}'>Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ</a></b></blockquote>\n\n<blockquote><emoji document_id=5465143921912846619>ğŸ’­</emoji> <b>Ğ—Ğ°Ğ¿Ğ¸Ñ‚:</b> <code>{prmpt}</code></blockquote>\n\n<blockquote><emoji document_id=5994544674604322765>ğŸ˜€</emoji> <b>ĞœĞ¾Ğ´ĞµĞ»ÑŒ:</b> <code>{mdl}</code></blockquote>",
		"args_err": "<blockquote><emoji document_id=5897846616966041652>â“</emoji> <b>Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ğ½Ğ½Ñ {prefix}dgpt/dimg <Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ> <Ğ·Ğ°Ğ¿Ğ¸Ñ‚></b></blockquote>",
		"query_err": "<blockquote><emoji document_id=5208434048753484584>â›”</emoji> <b>Ğ—Ğ°Ğ¿Ğ¸Ñ‚ Ğ½Ğµ Ğ¼Ğ¾Ğ¶Ğµ Ğ±ÑƒÑ‚Ğ¸ Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ–Ğ¼!</b></blockquote>",
		"server_err": "<blockquote><emoji document_id=5881702736843511327>âš ï¸</emoji> <b>ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° ÑĞµÑ€Ğ²ĞµÑ€Ğ°: {error}</b></blockquote>",
		"image_err": "<emoji document_id=5881702736843511327>âš ï¸</emoji> <b>ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ— Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ: {error}</b>",
		"models_list": "<blockquote><emoji document_id=5879841310902324730>ğŸ˜€</emoji><b>Ğ¢ĞµĞºÑÑ‚</b></blockquote>\n\n<blockquote>{txt_models}</blockquote>\n\n<blockquote><emoji document_id=5775949822993371030>ğŸ–¼</emoji> <b>Ğ—Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ</b></blockquote>\n\n<blockquote>{img_models}</blockquote>",
		"model_not_found": "<blockquote><emoji document_id=5208434048753484584>â›”</emoji> <b>ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°! Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ {prefix}dgmodels</b></blockquote>",
		"no_url": "ĞĞµ Ğ¾Ñ‚Ñ€Ğ¸Ğ¼Ğ°Ğ½Ğ¾ URL Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ",
		"no_server_respond": "ĞĞµĞ¼Ğ°Ñ” Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ– Ğ²Ñ–Ğ´ ÑĞµÑ€Ğ²ĞµÑ€Ğ°",
		"fetch_failed": "<blockquote><emoji document_id=5208663713539704322>ğŸ‘</emoji> <b>ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ğ¾Ñ‚Ñ€Ğ¸Ğ¼Ğ°Ñ‚Ğ¸ Ğ´Ğ°Ğ½Ñ–</b></blockquote>",
		"actual_version": "<blockquote> <emoji document_id=5208763618773978162>âœ…</emoji>Ğ£ Ğ²Ğ°Ñ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ° Ğ²ĞµÑ€ÑÑ–Ñ DevGPT ({ver})</b></blockquote>",
		"old_version": "<blockquote><emoji document_id=5875291072225087249>ğŸ“Š</emoji> Ğ£ Ğ²Ğ°Ñ Ğ·Ğ°ÑÑ‚Ğ°Ñ€Ñ–Ğ»Ğ° Ğ²ĞµÑ€ÑÑ–Ñ DevGPT ({ver}) </b>\n\n<emoji document_id=5879883461711367869>â¬‡ï¸</emoji> <b>ĞĞ¾Ğ²Ğ° Ğ²ĞµÑ€ÑÑ–Ñ: {new_ver} <b></blockquote>",
		"update_command": "<blockquote><emoji document_id=5877410604225924969>ğŸ”„</emoji> Ğ”Ğ»Ñ Ğ¾Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ Ğ²Ğ²ĞµĞ´Ñ–Ñ‚ÑŒ:</b> <code> {prefix}dlm {upd_file}</code></blockquote>",
		"ban": "<blockquote><emoji document_id=5208663713539704322>ğŸ‘</emoji> Ğ’Ğ°Ñ Ğ·Ğ°Ğ±Ğ°Ğ½ĞµĞ½Ğ¾! Ğ— Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ¸: {reason}</blockquote>",
	}

	strings_ru = {
		"wait": "<blockquote><emoji document_id=5994544674604322765>ğŸ˜€</emoji> <b>Ğ¡ĞµÑ€Ğ²ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, Ğ¿Ğ¾Ğ´Ğ¾Ğ¶Ğ´Ğ¸Ñ‚Ğµ...</b></blockquote>",
		"quest": "\n\n\n<blockquote><emoji document_id=5465143921912846619>ğŸ’­</emoji> <b>Ğ’Ğ°Ñˆ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ:</b> {args}</blockquote>",
		"quest_img": "<blockquote><b><emoji document_id=5877465816030515018>ğŸ˜€</emoji> Ğ¡ÑÑ‹Ğ»ĞºĞ°: <a href='{img_url}'>Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ</a></b></blockquote>\n\n<blockquote><emoji document_id=5465143921912846619>ğŸ’­</emoji> <b>Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ:</b> <code>{prmpt}</code></blockquote>\n\n<blockquote><emoji document_id=5994544674604322765>ğŸ˜€</emoji> <b>ĞœĞ¾Ğ´ĞµĞ»ÑŒ:</b> <code>{mdl}</code></blockquote>",
		"args_err": "<blockquote><emoji document_id=5897846616966041652>â“</emoji> <b>Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ {prefix}dgpt/dimg <Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ> <Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ></b></blockquote>",
		"query_err": "<blockquote><emoji document_id=5208434048753484584>â›”</emoji> <b>Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿ÑƒÑÑ‚Ñ‹Ğ¼!</b></blockquote>",
		"server_err": "<blockquote><emoji document_id=5881702736843511327>âš ï¸</emoji> <b>ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞµÑ€Ğ²ĞµÑ€Ğ°: {error}</b></blockquote>",
		"image_err": "<emoji document_id=5881702736843511327>âš ï¸</emoji> <b>ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {error}</b>",
		"models_list": "<blockquote><emoji document_id=5879841310902324730>ğŸ˜€</emoji><b>Ğ¢ĞµĞºÑÑ‚</b></blockquote>\n\n<blockquote>{txt_models}</blockquote>\n\n<blockquote><emoji document_id=5775949822993371030>ğŸ–¼</emoji> <b>Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ</b></blockquote>\n\n<blockquote>{img_models}</blockquote>",
		"model_not_found": "<blockquote><emoji document_id=5208434048753484584>â›”</emoji> <b>ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°! Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ {prefix}dgmodels</b></blockquote>",
		"no_url": "ĞĞµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½ URL Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
		"no_server_respond": "ĞĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¾Ñ‚ ÑĞµÑ€Ğ²ĞµÑ€Ğ°",
		"fetch_failed": "<blockquote><emoji document_id=5208663713539704322>ğŸ‘</emoji> <b>ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ</b></blockquote>",
		"actual_version": "<blockquote> <emoji document_id=5208763618773978162>âœ…</emoji>Ğ£ Ğ²Ğ°Ñ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ DevGPT ({ver})</b></blockquote>",
		"old_version": "<blockquote><emoji document_id=5875291072225087249>ğŸ“Š</emoji> Ğ£ Ğ²Ğ°Ñ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ DevGPT ({ver}) </b>\n\n<emoji document_id=5879883461711367869>â¬‡ï¸</emoji> <b>ĞĞ¾Ğ²Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ: {new_ver} <b></blockquote>",
		"update_command": "<blockquote><emoji document_id=5877410604225924969>ğŸ”„</emoji> Ğ”Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ:</b> <code> {prefix}dlm {upd_file}</code></blockquote>",
		"ban": "<blockquote><emoji document_id=5208663713539704322>ğŸ‘</emoji> Ğ’Ñ‹ Ğ·Ğ°Ğ±Ğ°Ğ½ĞµĞ½Ñ‹! ĞŸĞ¾ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğµ: {reason}</blockquote>",
	}

	async def client_ready(self, client, _):
		# self.server_url = "https://api.vysssotsky.ru"
		self.server_url = "https://api.vysssotsky.ru/"
		self.server_url_images = "https://v1.vysssotsky.ru/v1/{model_name}/generate"
		self.server_url_images_v2 = "https://v2.vysssotsky.ru/v1/generate"
		self.additional_server_url = "http://146.19.48.160:25701/generate_image"

		self.api_key = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c"

		self.repo = "https://raw.githubusercontent.com/Plovchikdeval/dev_modules/main"

		self._client = client
		self.prefix = self._client.loader.get_prefix()

		self.text_models = ["gpt-4", "gpt-4-turbo", "gpt-4o", "gpt-4o-mini", "o1-preview", "hermes-2-pro", "phi-2", "gemini-pro", "gemini-flash", "gemma-2b", "claude-3-haiku", "claude-3.5-sonnet", "blackboxai", "llava-13b", "openchat-3.5", "sonar-chat", "german-7b", "any-uncensored"]
		self.image_models = ["sd-3", "flux-pro", "flux-realism", "flux-anime", "flux-disney", "flux-pixel", "flux-4o", "any-dark", "flux"]
		self.additional_image_models = ["anything-v5", "dreamshaper-v6", "dreamshaper-v5", "meina-v9"]

	async def generate_text(self, message, args):
		model = args.split()[0]
		content = args.replace(model, "").strip()

		if len(content) <= 1:
			await utils.answer(message, self.strings("query_err"))
			return

		if model in self.text_models:
			try:
				payload = {
					"model": model,
					"messages": [{"role": "user", "content": content}],
					"max_tokens": 2048,
					"temperature": 0.7,
					"top_p": 1,
				}

				async with aiohttp.ClientSession() as session:
					async with session.post(f"{self.server_url}/v1/chat/completions", headers={"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}, data=json.dumps(payload)) as response:
						if response.status == 200:
							data = await response.json()
							answer = data.get("choices", [{}])[0].get("message", {}).get("content", self.strings("no_server_respond"))
							answer = f"<blockquote>{answer}</blockquote>"

							await utils.answer(message, answer + self.strings("quest").format(args=content))
						else:
							await utils.answer(message, self.strings("server_err").format(error=f"HTTP {response.status}"))
			except Exception as e:
				await utils.answer(message, self.strings("server_err").format(error=str(e)))
		else:
			await utils.answer(message, self.strings("model_not_found").format(prefix=self.prefix))


	async def generate_image(self, message, args):
		model = args.split()[0]
		prompt = args.replace(model, "").strip()

		if len(prompt) <= 1:
			await utils.answer(message, self.strings("query_err"))
			return

		if model in self.image_models:
			try:
				payload = {
					"prompt": prompt
				}

				async with aiohttp.ClientSession() as session:
					async with session.post(self.server_url_images.format(model_name=model), headers={"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}, data=json.dumps(payload)) as response:
						if response.status == 200:
							data = await response.json()
							image_url = data.get("data", [{}])[0].get("url", None)

							if image_url:
								try:
									async with session.get(image_url) as generated_image:
										file_name = "dgimage.png"
										with open(file_name,'wb') as file:
											file.write(await generated_image.read())

									await utils.answer_file(message, file_name, caption=(self.strings('quest_img').format(img_url=image_url, prmpt=prompt, mdl=model)))
								finally:
									if os.path.exists(file_name):
										os.remove(file_name)
							else:
								await utils.answer(message, self.strings("image_err").format(error=self.strings("no_url")))
						elif model not in ["sd-3", "any-dark"]:
							logger.warning("v1 API down! Trying to use v2 instead", exc_info=True)
							payload_v2 = {
								"model": model,
								"prompt": prompt
							}
							async with session.post(self.server_url_images_v2, headers={"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}, data=json.dumps(payload_v2)) as response_v2:
								if response_v2.status == 200:
									
									image_v2 = await response_v2.text()

									try:
										image_v2 = json.loads(image_v2)
										image_v2_url = image_v2.get("link")
									except json.JSONDecodeError:
										image_v2_url = image_v2.strip()

									async with session.get(image_v2_url) as image_v2_response:
										image_v2_response.raise_for_status()
										image_v2_content = io.BytesIO(await image_v2_response.read())
									await utils.answer_file(message, image_v2_content, caption=(self.strings('quest_img').format(img_url=image_v2_url, prmpt=prompt, mdl=model)))
								else:
									err_data = await response_v2.json()
									ban_reason = err_data.get("reason")
									await utils.answer(message, self.strings("ban").format(reason=ban_reason))
						elif response.status == 403:
							err_data = await response.json()
							ban_reason = err_data.get("reason")
							await utils.answer(message, self.strings("ban").format(reason=ban_reason))
						else:
							await utils.answer(message, self.strings("image_err").format(error=f"HTTP {response.status}"))

			except Exception as e:
				await utils.answer(message, self.strings("image_err").format(error=str(e)))
		elif model in self.additional_image_models:
			try:
				data = {
				"prompt": prompt,
				"model": model
				}
				headers = {"Content-Type": "application/json"}
				response = requests.post(self.additional_server_url, json=data, headers=headers)
				response.raise_for_status()
				result = response.json()
				image_url = result.get("image_url", "")
				image_response = requests.get(image_url)

				image = io.BytesIO(image_response.content)
				image.name = "generated_image.png"

				await utils.answer(message, image, caption=(self.strings("quest_img").format(img_url=image_url, prmpt=prompt, mdl=model)))
			except requests.exceptions.RequestException as e:
				await utils.answer(message, self.strings("image_err").format(error=e))
			except Exception as e:
				await utils.answer(message, self.strings("image_err").format(error=e))
		else:
			await utils.answer(message, self.strings("model_not_found").format(prefix=self.prefix))

	@loader.command(en_doc="Ask gpt for something", ru_doc="Ğ¡Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚Ğµ gpt Ğ¾ Ñ‡ĞµĞ¼-Ğ½Ğ¸Ğ±ÑƒĞ´ÑŒ", ua_doc="Ğ—Ğ°Ğ¿Ğ¸Ñ‚Ğ°Ğ¹Ñ‚Ğµ gpt Ğ¿Ñ€Ğ¾ Ñ‰Ğ¾ÑÑŒ")
	async def dgptcmd(self, message: Message):
		"""Ask gpt for something"""
		args = utils.get_args_raw(message)
		if not args:
			await utils.answer(message, self.strings("args_err").format(prefix=self.prefix))
			return

		await utils.answer(message, self.strings("wait"))

		await self.generate_text(message, args)

	@loader.command(en_doc="Generate image", ru_doc="Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ", ua_doc="Ğ—Ğ³ĞµĞ½ĞµÑ€ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ")
	async def dimg(self, message: Message):
		"""Generate image"""
		args = utils.get_args_raw(message)
		if not args:
			await utils.answer(message, self.strings("args_err").format(prefix=self.prefix))
			return

		await utils.answer(message, self.strings("wait"))

		await self.generate_image(message, args)

	@loader.command(en_doc="Display models list", ru_doc="ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", ua_doc="ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚Ğ¸ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹")
	async def dgmodels(self, message: Message):
		"""Display models list"""
		combined_list = self.image_models + self.additional_image_models
		t_mdl = '\n'.join(self.text_models)
		i_mdl = '\n'.join(combined_list)
		await utils.answer(message, self.strings("models_list").format(txt_models=t_mdl, img_models=i_mdl))

	@loader.command(en_doc="Check for updates", ru_doc="ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ", ua_doc="ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸ Ğ¾Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ")
	async def dgcheck(self, message: Message):
		"""Check for updates"""
		module_name = self.strings("name")
		module = self.lookup(module_name)
		sys_module = inspect.getmodule(module)

		local_file = io.BytesIO(sys_module.__loader__.data)
		local_file.name = f"{module_name}.py"
		local_file.seek(0)
		local_first_line = local_file.readline().strip().decode("utf-8")

		correct_version = sys_module.__version__
		correct_version_str = ".".join(map(str, correct_version))

		async with aiohttp.ClientSession() as session:
			async with session.get(f"{self.repo}/{local_file.name}") as response:
				if response.status == 200:
					remote_content = await response.text()
					remote_lines = remote_content.splitlines()

					new_version = remote_lines[0].split("=", 1)[1].strip().strip("()").replace(",", "").replace(" ", ".")
				else:
					await utils.answer(message, self.strings("fetch_failed"))
					return

		if local_first_line.replace(" ", "") == remote_lines[0].strip().replace(" ", ""):
			await utils.answer(message, self.strings("actual_version").format(ver=correct_version_str))
		else:
			update_message = self.strings("old_version").format(ver=correct_version_str, new_ver=new_version)
			update_message += self.strings("update_command").format(prefix=self.prefix, upd_file=f"{self.repo}/{local_file.name}")
			await utils.answer(message, update_message)

